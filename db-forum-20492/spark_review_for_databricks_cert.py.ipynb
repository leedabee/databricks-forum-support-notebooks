{"cells":[{"cell_type":"markdown","source":["# Databricks Developer Certification (python)\n\n<https://academy.databricks.com/exam/crt020-python>\n\n## Expectation of Knowledge\n\n### Spark Architecture Components\n\nCandidates are expected to be familiar with the following architectural components and their relationship to each other:\n\n- Driver\n- Executor\n- Cores/Slots/Threads\n- Partitions\n\n### Spark Execution\n\nCandidates are expected to be familiar with Sparkâ€™s execution model and the breakdown between the different elements:\n\n- Jobs\n- Stages\n- Tasks\n\n### Spark Concepts\n\nCandidates are expected to be familiar with the following concepts:\n\n- Caching\n- Shuffling\n- Partitioning\n- Wide vs. Narrow Transformations\n- DataFrame Transformations vs. Actions vs. Operations\n- High-level Cluster Configuration\n\n### DataFrames API\n- SparkContext\n  - Candidates are expected to know how to use the SparkContext to control basic\n    configuration settings such as spark.sql.shuffle.partitions.\n\n- SparkSession\n  - Candidates are expected to know how to:\n    - Create a DataFrame/Dataset from a collection (e.g. list or set)\n    - Create a DataFrame for a range of numbers\n    - Access the DataFrameReaders\n    - Register User Defined Functions (UDFs)\n    \n- DataFrameReader\n  - Candidates are expected to know how to:\n    - Read data for the \"core\" data formats (CSV, JSON, JDBC, ORC, Parquet, text and tables)\n    - How to configure options for specific formats\n    - How to read data from non-core formats using format() and load()\n    - How to specify a DDL-formatted schema\n    - How to construct and specify a schema using the StructType classes\n\n- DataFrameWriter\n  - Candidates are expected to know how to:\n    - Write data to the \"core\" data formats (csv, json, jdbc, orc, parquet, text and tables)\n    - Overwriting existing files\n    - How to configure options for specific formats\n    - How to write a data source to 1 single file or N separate files\n    - How to write partitioned data\n    - How to bucket data by a given set of columns\n\n- DataFrame\n  - Have a working understanding of every action such as take(), collect(), and foreach()\n  - Have a working understanding of the various transformations and how they\n    work such as producing a distinct set, filtering data, repartitioning and\n    coalescing, performing joins and unions as well as producing aggregates\n  - Know how to cache data, specifically to disk, memory or both\n  - Know how to uncache previously cached data\n  - Converting a DataFrame to a global or temp view.\n  - Applying hints\n  \n- Row & Column\n  - Candidates are expected to know how to work with row and columns to successfully extract data from a DataFrame\n\n- Spark SQL Functions\n  - When instructed what to do, candidates are expected to be able to employ the\n    multitude of Spark SQL functions. Examples include, but are not limited to:\n    - Aggregate functions: getting the first or last item from an array or computing the min and max values of a column.\n    - Collection functions: testing if an array contains a value, exploding or flattening data.\n    - Date time functions: parsing strings into timestamps or formatting timestamps into strings\n    - Math functions: computing the cosign, floor or log of a number\n    - Misc functions: converting a value to crc32, md5, sha1 or sha2\n    - Non-aggregate functions: creating an array, testing if a column is null, not-null, nan, etc\n    - Sorting functions: sorting data in descending order, ascending order, and sorting with proper null handling\n    - String functions: applying a provided regular expression, trimming string and extracting substrings.\n    - UDF functions: employing a UDF function.\n    - Window functions: computing the rank or dense rank."],"metadata":{}},{"cell_type":"markdown","source":["## SparkContext\n\nCandidates are expected to know how to use the SparkContext to control basic configuration settings such as spark.sql.shuffle.partitions."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.shuffle.partitions\", 6)\nspark.conf.set(\"spark.executor.memory\", \"2g\")"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["print(spark.conf.get(\"spark.sql.shuffle.partitions\"), \",\", spark.conf.get(\"spark.executor.memory\"))"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["%sql\nSET spark.sql.shuffle.partitions = 8;\nSET spark.executor.memory = 4g;"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["print(spark.conf.get(\"spark.sql.shuffle.partitions\"), \",\", spark.conf.get(\"spark.executor.memory\"))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["## SparkSession\n\nCandidates are expected to know how to:\n- Create a DataFrame/Dataset from a collection (e.g. list or set)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import IntegerType\n\nlist_df = spark.createDataFrame([1, 2, 3, 4], IntegerType())\ndisplay(my_list_df)"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["- Create a DataFrame for a range of numbers"],"metadata":{}},{"cell_type":"code","source":["ints_df = spark.range(1000).toDF(\"number\")\ndisplay(ints_df)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["- Access the DataFrameReaders"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.csv('/FileStore/tables/input.csv', inferSchema=True)\n# spark.read.parquet()\n# spark.read.json()\n# spark.read.format().open()"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["- Register User Defined Functions (UDFs)"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.functions import udf\nfrom pyspark.sql.types import IntegerType\n\ndef power3(value):\n  return value ** 3\n\nspark.udf.register(\"power3py\", power3, IntegerType())"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"code","source":["power3udf = udf(power3, IntegerType())\npower3_ints_df = ints_df.select(\"number\", power3udf(\"number\").alias(\"power3\"))\ndisplay(power3_ints_df)"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"code","source":["spark.range(1, 20).registerTempTable(\"test\")"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"code","source":["%sql select id, power3py(id) as power3 from test"],"metadata":{},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["## DataFrameReader"],"metadata":{}},{"cell_type":"markdown","source":["- Read data for the \"core\" data formats (CSV, JSON, JDBC, ORC, Parquet, text and tables)"],"metadata":{}},{"cell_type":"code","source":["data_file = \"/FileStore/tables/sales.csv\"\n\ndf = spark.read.csv(data_file)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["- How to configure options for specific formats"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.csv(data_file, header=True, inferSchema=True)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["- How to read data from non-core formats using format() and load()"],"metadata":{}},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"inferSchema\",\"true\").option(\"header\",\"true\").load(data_file)"],"metadata":{},"outputs":[],"execution_count":24},{"cell_type":"markdown","source":["- How to construct and specify a schema using the StructType classes"],"metadata":{}},{"cell_type":"code","source":["from pyspark.sql.types import StructField, StructType, StringType, LongType\n\nmyManualSchema = StructType([\n  StructField(\"field1\", StringType()),\n  StructField(\"field2\", StringType()),\n  StructField(\"field3\", StringType())\n])\n\ndf3 = spark.read.format(\"csv\").schema(myManualSchema).option(\"header\",\"true\").load(data_file)\ndf3.show()"],"metadata":{},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":["- How to specify a DDL-formatted schema"],"metadata":{}},{"cell_type":"markdown","source":["## DataFrameWriter"],"metadata":{}},{"cell_type":"markdown","source":["- Write data to the \"core\" data formats (csv, json, jdbc, orc, parquet, text and tables)"],"metadata":{}},{"cell_type":"code","source":["df.write.parquet('myparquetfile')\ndf.write.saveAsTable('mytable')"],"metadata":{},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":["- Overwriting existing files"],"metadata":{}},{"cell_type":"code","source":["#df.write.mode(\"overwrite\")"],"metadata":{},"outputs":[],"execution_count":32},{"cell_type":"markdown","source":["- How to configure options for specific formats"],"metadata":{}},{"cell_type":"code","source":["csvFile = (spark.read.format(\"csv\")\n  .option(\"header\", \"true\")\n  .option(\"inferSchema\", \"true\")\n  .load(data_file))\n\ncsvFile.write.format(\"csv\").mode(\"overwrite\").option(\"sep\", \"\\t\").save(\"my-tsv-file.tsv\")"],"metadata":{},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":["- How to write a data source to 1 single file or N separate files"],"metadata":{}},{"cell_type":"code","source":["# df.coalesce(1) \n# df.repartition(1)"],"metadata":{},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":["- How to write partitioned data\n- How to bucket data by a given set of columns"],"metadata":{}},{"cell_type":"code","source":["(df\n    .write\n    .partitionBy(\"ProductKey\")\n    .bucketBy(42, \"OrderDateKey\")\n    .saveAsTable(\"orders_partitioned_bucketed\"))"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["## DataFrame"],"metadata":{}},{"cell_type":"markdown","source":["- Have a working understanding of every action such as take(), collect(), and foreach()"],"metadata":{}},{"cell_type":"markdown","source":["- Have a working understanding of the various transformations and how they work such as producing a distinct set, filtering data, repartitioning and coalescing, performing joins and unions as well as producing aggregates"],"metadata":{}},{"cell_type":"markdown","source":["- Know how to cache data, specifically to disk, memory or both"],"metadata":{}},{"cell_type":"code","source":["from pyspark.storagelevel import StorageLevel\n\ndf.persist(StorageLevel.MEMORY_AND_DISK)"],"metadata":{},"outputs":[],"execution_count":43},{"cell_type":"markdown","source":["- Know how to uncache previously cached data"],"metadata":{}},{"cell_type":"code","source":["df.unpersist()"],"metadata":{},"outputs":[],"execution_count":45},{"cell_type":"markdown","source":["- Converting a DataFrame to a global or temp view"],"metadata":{}},{"cell_type":"code","source":["# df.createOrReplaceTempView(\"<table-name>\")"],"metadata":{},"outputs":[],"execution_count":47},{"cell_type":"markdown","source":["- Applying hints"],"metadata":{}},{"cell_type":"code","source":["# df.join(df2.hint(\"broadcast\"), \"name\").show()"],"metadata":{},"outputs":[],"execution_count":49}],"metadata":{"name":"spark_review_for_databricks_cert.py","notebookId":521888783067570},"nbformat":4,"nbformat_minor":0}
